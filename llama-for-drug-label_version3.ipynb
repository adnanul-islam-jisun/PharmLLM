{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10612235,"sourceType":"datasetVersion","datasetId":6569925},{"sourceId":120000,"sourceType":"modelInstanceVersion","modelInstanceId":100931,"modelId":121027}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install trl==0.12.2\n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install -U huggingface_hub ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n    Trainer\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\n\nimport os, torch, wandb\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:50:52.417009Z","iopub.execute_input":"2025-01-29T19:50:52.417328Z","iopub.status.idle":"2025-01-29T19:51:00.038877Z","shell.execute_reply.started":"2025-01-29T19:50:52.417302Z","shell.execute_reply":"2025-01-29T19:51:00.038136Z"}},"outputs":[{"name":"stderr","text":"2025-01-29 19:50:55.575626: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-29 19:50:55.575690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-29 19:50:55.577324: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    capability = torch.cuda.get_device_capability()[0]\n\n    if capability >= 8:\n        !pip install -qqq flash-attn\n        torch_dtype = torch.bfloat16\n        attn_implementation = \"flash_attention_2\"\n        print(\"Using CUDA with FlashAttention\")\n    else:  # For T4 GPUs (compute capability 7.5)\n        torch_dtype = torch.float16\n        attn_implementation = \"sdpa\"  # SDPA is better for older GPUs\n        print(\"Using CUDA with SDPA (Scaled Dot-Product Attention)\")\n\nelse:\n    device = torch.device(\"cpu\")\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"\n    print(\"Using CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:51:00.040451Z","iopub.execute_input":"2025-01-29T19:51:00.040811Z","iopub.status.idle":"2025-01-29T19:51:00.122134Z","shell.execute_reply.started":"2025-01-29T19:51:00.040750Z","shell.execute_reply":"2025-01-29T19:51:00.121249Z"}},"outputs":[{"name":"stdout","text":"Using CUDA with SDPA (Scaled Dot-Product Attention)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3.2/transformers/1b/1\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:51:08.162550Z","iopub.execute_input":"2025-01-29T19:51:08.162907Z","iopub.status.idle":"2025-01-29T19:51:08.167111Z","shell.execute_reply.started":"2025-01-29T19:51:08.162879Z","shell.execute_reply":"2025-01-29T19:51:08.166065Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# QLoRA config -- 4bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:51:18.079558Z","iopub.execute_input":"2025-01-29T19:51:18.080326Z","iopub.status.idle":"2025-01-29T19:51:35.758077Z","shell.execute_reply.started":"2025-01-29T19:51:18.080282Z","shell.execute_reply":"2025-01-29T19:51:35.757324Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:51:35.759406Z","iopub.execute_input":"2025-01-29T19:51:35.759668Z","iopub.status.idle":"2025-01-29T19:51:35.765323Z","shell.execute_reply.started":"2025-01-29T19:51:35.759645Z","shell.execute_reply":"2025-01-29T19:51:35.764350Z"}},"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"if tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\nif model.config.pad_token_id is None:\n    model.config.pad_token_id = model.config.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:51:35.766333Z","iopub.execute_input":"2025-01-29T19:51:35.766697Z","iopub.status.idle":"2025-01-29T19:51:35.773255Z","shell.execute_reply.started":"2025-01-29T19:51:35.766676Z","shell.execute_reply":"2025-01-29T19:51:35.772433Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(\n#     base_model,\n#     quantization_config=bnb_config,\n#     device_map=\"auto\",\n#     attn_implementation=attn_implementation\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Before fine-tuning with the drug labels","metadata":{}},{"cell_type":"code","source":"# pipe = pipeline(\n#     \"text-generation\",\n#     model=model,\n#     tokenizer=tokenizer,\n#     torch_dtype=torch_dtype,\n#     device_map=\"auto\",\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# input_data = {\n#   \"brand_name\": \"PRISMASOL BGK2/0\",\n#   \"generic_name\": \"MAGNESIUM CHLORIDE, DEXTROSE MONOHYDRATE, LACTIC ACID, SODIUM CHLORIDE, SODIUM BICARBONATE AND POTASSIUM CHLORIDE\",\n#   \"query\": \"Does the drug PRISMASOL BGK2/0 have any adverse reactions?\"\n# }\n\n# messages = [\n#     {\"role\": \"user\", \"content\": f\"Brand Name: {input_data['brand_name']}\\nGeneric Name: {input_data['generic_name']}\\n\\nQuery: {input_data['query']}\"}\n# ]\n# messages = [{\"role\": \"system\", \"content\": \"You are a helpful medical assistant.\"}] + messages\n\n# prompt = tokenizer.apply_chat_template(\n#     messages, tokenize=False, add_generation_prompt=True\n# )\n\n# outputs = pipe(prompt, max_new_tokens=1000, do_sample=True)\n\n# print(outputs[0][\"generated_text\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# '''\n#     Expected Adverse Reaction:\n# '''\n# expected = {\n#     \"adverse_reactions\": [\n#         \"6 ADVERSE REACTIONS The following adverse reactions have been identified during postapproval use with these or other similar products and therefore may occur with use of PHOXILLUM or PRISMASOL. Because these reactions are reported voluntarily from a population of uncertain size, it is not always possible to reliably estimate their frequency or establish a causal relationship to drug exposure. \\u2022 Metabolic acidosis \\u2022 Hypotension \\u2022 Acid-base disorders \\u2022 Electrolyte imbalance including calcium ionized increased (reported in PRISMASOL solutions containing calcium), hyperphosphatemia, and hypophosphatemia \\u2022 Fluid imbalance\"\n#       ],\n# }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adverse Reaction","metadata":{}},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)\nmodules","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:51:44.087815Z","iopub.execute_input":"2025-01-29T19:51:44.088581Z","iopub.status.idle":"2025-01-29T19:51:44.096475Z","shell.execute_reply.started":"2025-01-29T19:51:44.088553Z","shell.execute_reply":"2025-01-29T19:51:44.095529Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['gate_proj', 'o_proj', 'v_proj', 'down_proj', 'up_proj', 'k_proj', 'q_proj']"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, PeftType, TaskType\n\n# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules\n)\n\n# Apply PEFT to the model\nmodel = get_peft_model(model, peft_config)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:51:49.771788Z","iopub.execute_input":"2025-01-29T19:51:49.772584Z","iopub.status.idle":"2025-01-29T19:51:50.033322Z","shell.execute_reply.started":"2025-01-29T19:51:49.772556Z","shell.execute_reply":"2025-01-29T19:51:50.032428Z"}},"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 2048)\n        (layers): ModuleList(\n          (0-15): 16 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/drug-label-filtered/adverse_reaction.json\"\n\ndataset = load_dataset('json', data_files=dataset_path)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef preprocess_data(examples):\n    inputs = [\n        f\"Input: {input_text} Response: {response_text}\"\n        for input_text, response_text in zip(examples[\"input_text\"], examples[\"response_text\"])\n    ]\n    # Tokenize\n    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n    return model_inputs\n\n# Apply preprocessing\ntokenized_dataset = dataset.map(preprocess_data, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:51:57.865670Z","iopub.execute_input":"2025-01-29T19:51:57.866012Z","iopub.status.idle":"2025-01-29T19:56:11.695144Z","shell.execute_reply.started":"2025-01-29T19:51:57.865987Z","shell.execute_reply":"2025-01-29T19:56:11.694209Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e4b2b9ec2d645b4a3df299244cba749"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/117619 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"271ff28286ed4e81b069bddc6a05faa3"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the dataset into training and evaluation sets\ntrain_test_split_ratio = 0.8\nsplit_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=1 - train_test_split_ratio, seed=42)\n\n# Access train and evaluation datasets\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]\n\n# Print dataset sizes\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Evaluation dataset size: {len(eval_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:56:11.696889Z","iopub.execute_input":"2025-01-29T19:56:11.697152Z","iopub.status.idle":"2025-01-29T19:56:11.742599Z","shell.execute_reply.started":"2025-01-29T19:56:11.697130Z","shell.execute_reply":"2025-01-29T19:56:11.741698Z"}},"outputs":[{"name":"stdout","text":"Train dataset size: 94095\nEvaluation dataset size: 23524\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(model.config.max_position_embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training arguments\nbatch_size = 4\ntraining_arguments = TrainingArguments(\n    output_dir=\"Pharmllm\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=2,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:56:11.743505Z","iopub.execute_input":"2025-01-29T19:56:11.743750Z","iopub.status.idle":"2025-01-29T19:56:12.395811Z","shell.execute_reply.started":"2025-01-29T19:56:11.743725Z","shell.execute_reply":"2025-01-29T19:56:12.394870Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"input_text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T19:57:37.051867Z","iopub.execute_input":"2025-01-29T19:57:37.052927Z","execution_failed":"2025-01-30T07:44:47.638Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdshahadat10\u001b[0m (\u001b[33mdshahadat10-united-international-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250129_195857-3769fk9i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dshahadat10-united-international-university/huggingface/runs/3769fk9i' target=\"_blank\">Pharmllm</a></strong> to <a href='https://wandb.ai/dshahadat10-united-international-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dshahadat10-united-international-university/huggingface' target=\"_blank\">https://wandb.ai/dshahadat10-united-international-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dshahadat10-united-international-university/huggingface/runs/3769fk9i' target=\"_blank\">https://wandb.ai/dshahadat10-united-international-university/huggingface/runs/3769fk9i</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='17479' max='47048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [17479/47048 11:45:44 < 19:54:01, 0.41 it/s, Epoch 0.74/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>9410</td>\n      <td>0.344200</td>\n      <td>0.365320</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"trainer.model.save_pretrained(\"pharmllam_adverse\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-30T07:44:47.639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# py file cell","metadata":{}},{"cell_type":"code","source":"# from transformers import (\n#     AutoModelForCausalLM,\n#     AutoTokenizer,\n#     BitsAndBytesConfig,\n#     HfArgumentParser,\n#     TrainingArguments,\n#     pipeline,\n#     logging,\n#     Trainer\n# )\n# from peft import (\n#     LoraConfig,\n#     PeftModel,\n#     prepare_model_for_kbit_training,\n#     get_peft_model,\n#     TaskType\n# )\n\n# import os, torch, wandb\n# from datasets import load_dataset\n# from sklearn.model_selection import train_test_split\n# from trl import SFTTrainer\n\n\n# # Set torch dtype and attention implementation\n# if torch.cuda.get_device_capability()[0] >= 8:\n#     !pip install -qqq flash-attn\n#     torch_dtype = torch.bfloat16\n#     attn_implementation = \"flash_attention_2\"\n#     print(\"cuda\")\n# else:\n#     torch_dtype = torch.float16\n#     attn_implementation = \"eager\"\n#     print(\"cpu\")\n\n# base_model = \"/kaggle/input/llama-3.2/transformers/1b/1\"\n\n# # QLoRA config -- 4bit quantization\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch_dtype,\n#     bnb_4bit_use_double_quant=True,\n# )\n\n# # Load model\n# model = AutoModelForCausalLM.from_pretrained(\n#     base_model,\n#     quantization_config=bnb_config,\n#     device_map=\"auto\",\n#     attn_implementation=attn_implementation\n# )\n\n# # Load tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n\n# if tokenizer.pad_token_id is None:\n#     tokenizer.pad_token_id = tokenizer.eos_token_id\n# if model.config.pad_token_id is None:\n#     model.config.pad_token_id = model.config.eos_token_id\n\n\n# import bitsandbytes as bnb\n\n# def find_all_linear_names(model):\n#     cls = bnb.nn.Linear4bit\n#     lora_module_names = set()\n#     for name, module in model.named_modules():\n#         if isinstance(module, cls):\n#             names = name.split('.')\n#             lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n#     if 'lm_head' in lora_module_names:  # needed for 16 bit\n#         lora_module_names.remove('lm_head')\n#     return list(lora_module_names)\n\n# modules = find_all_linear_names(model)\n\n# # LoRA config\n# peft_config = LoraConfig(\n#     r=16,\n#     lora_alpha=32,\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n#     target_modules=modules\n# )\n\n# # Apply PEFT to the model\n# model = get_peft_model(model, peft_config)\n# print(model)\n\n# dataset_path = \"/kaggle/input/drug-label-filtered/adverse_reaction.json\"\n\n# dataset = load_dataset('json', data_files=dataset_path)\n# tokenizer.pad_token = tokenizer.eos_token\n\n# def preprocess_data(examples):\n#     inputs = [\n#         f\"Input: {input_text} Response: {response_text}\"\n#         for input_text, response_text in zip(examples[\"input_text\"], examples[\"response_text\"])\n#     ]\n#     # Tokenize\n#     model_inputs = tokenizer(inputs, max_length=5120, truncation=True, padding=\"max_length\")\n#     return model_inputs\n\n# # Apply preprocessing\n# tokenized_dataset = dataset.map(preprocess_data, batched=True)\n\n# # Split the dataset into training and evaluation sets\n# train_test_split_ratio = 0.8\n# split_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=1 - train_test_split_ratio, seed=42)\n\n# # Access train and evaluation datasets\n# train_dataset = split_dataset[\"train\"]\n# eval_dataset = split_dataset[\"test\"]\n\n# # Print dataset sizes\n# print(f\"Train dataset size: {len(train_dataset)}\")\n# print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n\n# # Define training arguments\n# batch_size = 4\n# training_arguments = TrainingArguments(\n#     output_dir=\"Pharmllm\",\n#     per_device_train_batch_size=2,\n#     per_device_eval_batch_size=2,\n#     gradient_accumulation_steps=2,\n#     optim=\"paged_adamw_32bit\",\n#     num_train_epochs=1,\n#     eval_strategy=\"steps\",\n#     eval_steps=0.2,\n#     logging_steps=1,\n#     warmup_steps=10,\n#     logging_strategy=\"steps\",\n#     learning_rate=2e-4,\n#     fp16=False,\n#     bf16=False,\n#     group_by_length=True,\n#     report_to=\"wandb\"\n# )\n\n\n# trainer = SFTTrainer(\n#     model=model,\n#     train_dataset=train_dataset,\n#     eval_dataset=eval_dataset,\n#     peft_config=peft_config,\n#     max_seq_length= 512,\n#     dataset_text_field=\"input_text\",\n#     tokenizer=tokenizer,\n#     args=training_arguments,\n#     packing= False,\n# )\n\n# trainer.train()\n\n# trainer.model.save_pretrained(\"pharmllam_adverse_reaction\")\n\n# #wandb api key: e94acafecf7a152ebfc203f27e1d857e1036edeb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}